<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Midterm Exam: Advanced Machine Learning - SOLUTIONS</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
            max-width: 800px;
            margin-left: auto;
            margin-right: auto;
        }
        h1, h2, h3 {
            color: #2c3e50;
        }
        h1 {
            text-align: center;
        }
        h2 {
            border-bottom: 2px solid #3498db;
            padding-bottom: 5px;
        }
        h3 {
            margin-top: 20px;
        }
        p, li {
            color: #333;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: center;
        }
        th {
            background-color: #f2f2f2;
        }
        .instructions {
            background-color: #ecf0f1;
            padding: 10px;
            border-radius: 5px;
            margin-bottom: 20px;
        }
        .section {
            margin-bottom: 30px;
        }
        .answer {
            background-color: #e8f6f3; /* Light green background for answers */
            padding: 10px;
            border-left: 4px solid #1abc9c; /* Green left border */
            margin-top: 10px;
            margin-bottom: 15px;
        }
        code {
            background-color: #f4f4f4;
            padding: 2px 4px;
            border-radius: 3px;
            font-family: monospace;
        }
    </style>
</head>
<body>
    <h1>Midterm Exam: Advanced Machine Learning (answers courtesy of various LLMs, with corrections by the crazy prof)</h1>
    <p><strong>Date:</strong> April 02, 2025<br>
       <strong>Duration:</strong> 2 hours</p>

    <div class="instructions">
        <h3>Instructions:</h3>
        <ul>
            <li>Answer all questions.</li>
            <li>Show your work for partial credit where applicable.</li>
        </ul>
    </div>

    <div class="section">
        <h2>Section 1: Classification Metrics</h2>

        <h3>1. Short Answer</h3>
        <ol type="a">
            <li>Define precision, recall, and F1-score in the context of binary classification.</li>
            <div class="answer">
                <p><strong>Precision:</strong> Measures the accuracy of positive predictions. It is the ratio of correctly predicted positive observations (True Positives, TP) to the total predicted positive observations (TP + False Positives, FP). Formula: <code>Precision = TP / (TP + FP)</code>. It answers the question: "Of all instances predicted as positive, how many were actually positive?"</p>
                <p><strong>Recall (Sensitivity or True Positive Rate):</strong> Measures the ability of the classifier to find all the positive samples. It is the ratio of correctly predicted positive observations (TP) to all observations in the actual positive class (TP + False Negatives, FN). Formula: <code>Recall = TP / (TP + FN)</code>. It answers the question: "Of all actual positive instances, how many did the classifier correctly identify?"</p>
                <p><strong>F1-Score:</strong> The harmonic mean of Precision and Recall. It provides a balance between the two metrics, especially useful when the class distribution is uneven. Formula: <code>F1 = 2 * (Precision * Recall) / (Precision + Recall)</code>. It tries to find an optimal blend of precision and recall.</p>
            </div>

            <li>Explain a scenario where precision might be more important than recall, and vice versa. Provide a concrete example for each.</li>
            <div class="answer">
                <p><strong>Scenario prioritizing Precision over Recall:</strong><br>
                   <em>Importance:</em> Minimizing False Positives (FP) is crucial.<br>
                   <em>Example:</em> Email Spam Detection. A false positive occurs when a legitimate email (ham) is classified as spam. This is highly undesirable as the user might miss important information. A false negative (spam classified as ham) is less critical, as the user can simply delete the spam email from their inbox. Therefore, we want the classifier to be very precise when it marks an email as spam (high precision).
                </p>
                <p><strong>Scenario prioritizing Recall over Precision:</strong><br>
                   <em>Importance:</em> Minimizing False Negatives (FN) is crucial.<br>
                   <em>Example:</em> Medical Diagnosis for a serious, contagious disease (e.g., detecting a specific virus). A false negative occurs when a patient who actually has the disease is tested negative. This is extremely dangerous as the patient might not receive treatment and could spread the disease. A false positive (healthy patient tested positive) is less critical, as further tests can be conducted to confirm, causing some inconvenience but preventing a major health risk. Therefore, we want the classifier to identify as many actual cases as possible (high recall).
                </p>
            </div>

            <li>Why might the F1-score be preferred over accuracy as a metric in imbalanced datasets?</li>
            <div class="answer">
                <p>Accuracy measures the overall correctness: <code>Accuracy = (TP + TN) / (TP + FP + TN + FN)</code>.</p>
                <p>In imbalanced datasets, where one class (usually the negative class) vastly outnumbers the other (positive class), a classifier can achieve high accuracy by simply predicting the majority class for all instances. For example, if 99% of data points belong to the negative class, a classifier predicting "negative" for everything achieves 99% accuracy but is useless for identifying the positive class.</p>
                <p>The F1-score, being the harmonic mean of precision and recall, focuses on the performance on the positive class (TP, FP, FN). It requires both precision and recall to be reasonably high for the F1-score to be high. Therefore, F1-score provides a better measure of a classifier's ability to handle the minority class, which is often the class of interest in imbalanced scenarios, making it preferable over simple accuracy.</p>
            </div>
        </ol>

        <h3>2. Problem Solving</h3>
        <p>Consider a binary classifier with the following confusion matrix:</p>
        <ul>
            <li>True Positives (TP) = 50</li>
            <li>False Positives (FP) = 20</li>
            <li>True Negatives (TN) = 900</li>
            <li>False Negatives (FN) = 30</li>
        </ul>
        <ol type="a">
            <li>Compute the precision, recall, and F1-score.</li>
            <div class="answer">
                <p><strong>Precision:</strong><br>
                   <code>Precision = TP / (TP + FP) = 50 / (50 + 20) = 50 / 70 ≈ 0.714</code>
                </p>
                <p><strong>Recall:</strong><br>
                   <code>Recall = TP / (TP + FN) = 50 / (50 + 30) = 50 / 80 = 0.625</code>
                </p>
                <p><strong>F1-Score:</strong><br>
                   <code>F1 = 2 * (Precision * Recall) / (Precision + Recall)</code><br>
                   <code>F1 = 2 * (0.714 * 0.625) / (0.714 + 0.625)</code><br>
                   <code>F1 = 2 * 0.44625 / 1.339 ≈ 0.8925 / 1.339 ≈ 0.667</code>
                </p>
            </div>

            <li>If the cost of a false positive is 5 times higher than the cost of a false negative, design a custom metric to evaluate this classifier and compute its value.</li>
            <div class="answer">
                <p>Let Cost(FP) be the cost of a False Positive and Cost(FN) be the cost of a False Negative.</p>
                <p>We are given: Cost(FP) = 5 * Cost(FN). Let's assign a base cost, say Cost(FN) = 1 unit. Then Cost(FP) = 5 units.</p>
                <p>A simple custom metric could be the <strong>Total Cost</strong> associated with the classifier's errors. A lower total cost is better.</p>
                <p><code>Total Cost = (Number of FP * Cost(FP)) + (Number of FN * Cost(FN))</code><br>
                   <code>Total Cost = (20 * 5) + (30 * 1)</code><br>
                   <code>Total Cost = 100 + 30 = 130</code>
                </p>
                <p>Alternatively, one could define a weighted accuracy or a modification of the F-score (like F-beta score with beta chosen based on costs), but Total Cost directly reflects the economic impact.</p>
                <p><strong>Custom Metric Value (Total Cost): 130</strong></p>
            </div>

            <li>How would you adjust the decision threshold to prioritize recall over precision? Explain qualitatively.</li>
            <div class="answer">
                <p>To prioritize recall over precision, you should <strong>lower</strong> the decision threshold of the classifier.</p>
                <p><em>Explanation:</em> Most binary classifiers output a probability score (or a score analogous to probability) for an instance belonging to the positive class. The decision threshold determines the cutoff point (e.g., 0.5) above which an instance is classified as positive.</p>
                <ul>
                    <li>Lowering the threshold means that instances with lower predicted probabilities will now be classified as positive.</li>
                    <li>This leads to classifying more instances as positive overall.</li>
                    <li>As a result, more of the actual positive instances (TP + FN) are likely to be captured (increase in TP, decrease in FN), thus increasing <strong>recall</strong> (TP / (TP + FN)).</li>
                    <li>However, this also means that more actual negative instances might be incorrectly classified as positive (increase in FP), thus decreasing <strong>precision</strong> (TP / (TP + FP)).</li>
                </ul>
                <p>Therefore, lowering the threshold trades off lower precision for higher recall.</p>
            </div>
        </ol>
    </div>

    <div class="section">
        <h2>More on Classification Metrics</h2>
        <ol type="a">
            <li>Describe the beta distribution and explain why it is suitable for modeling the accuracy of a machine learning classifier. Describe its parameters.</li>
            <div class="answer">
                <p><strong>Beta Distribution:</strong> The beta distribution is a continuous probability distribution defined on the interval [0, 1]. It is parameterized by two positive shape parameters, denoted by α (alpha) and β (beta).</p>
                <p><strong>Suitability for Modeling Accuracy:</strong>
                    <ul>
                        <li>The range [0, 1] matches the natural range of accuracy (which is a proportion or probability).</li>
                        <li>It is flexible and can model a wide variety of shapes (uniform, U-shaped, bell-shaped, skewed) depending on the values of α and β, reflecting different states of belief about the accuracy.</li>
                        <li>It has a natural interpretation in Bayesian statistics as a conjugate prior for the Bernoulli, binomial, negative binomial, and geometric distributions. This means if we start with a beta prior belief about accuracy and observe new data (correct/incorrect predictions), the updated belief (posterior distribution) is also a beta distribution, making updates straightforward.</li>
                    </ul>
                </p>
                <p><strong>Parameters (α and β):</strong>
                    <ul>
                        <li>α (alpha) and β (beta) are positive real numbers that control the shape of the distribution.</li>
                        <li>In the context of modeling accuracy using a Bayesian approach (often with a Beta(1,1) uniform prior), α can be interpreted as (number of successes + 1) and β as (number of failures + 1). More generally, α relates to the count of successes (e.g., correct classifications) and β relates to the count of failures (e.g., incorrect classifications).</li>
                        <li>Larger values of α relative to β shift the distribution's mass towards 1 (higher accuracy).</li>
                        <li>Larger values of β relative to α shift the distribution's mass towards 0 (lower accuracy).</li>
                        <li>Larger values of both α and β lead to a narrower distribution, indicating greater certainty about the accuracy estimate. The sum α + β can be thought of as representing the "equivalent sample size" or strength of the belief.</li>
                    </ul>
                </p>
            </div>
            <li>How do the shape parameters (α and β) of the beta distribution relate to the number of correct and incorrect predictions?</li>
            <div class="answer">
                <p>In a common Bayesian framework for modeling the success probability (like accuracy) of a process with binary outcomes (correct/incorrect), the beta distribution parameters α and β are directly related to the counts of successes and failures observed.</p>
                <p>If we start with a prior belief represented by Beta(α₀, β₀) (e.g., a Beta(1, 1) uniform prior, meaning α₀=1, β₀=1), and then observe `k` correct predictions (successes) and `n-k` incorrect predictions (failures) out of `n` total trials, the updated (posterior) distribution for the accuracy is:</p>
                <p><code>Beta(α_posterior, β_posterior) = Beta(α₀ + k, β₀ + n - k)</code></p>
                <p>So, in this interpretation:
                    <ul>
                        <li><code>α</code> (specifically, α - α₀) represents the number of <strong>correct</strong> predictions observed.</li>
                        <li><code>β</code> (specifically, β - β₀) represents the number of <strong>incorrect</strong> predictions observed.</li>
                    </ul>
                </p>
                 <p>Essentially, α is influenced by successes, pulling the distribution towards 1, while β is influenced by failures, pulling it towards 0. The total number of observations (plus prior pseudo-counts α₀ + β₀ - 2) influences the concentration or certainty (variance) of the distribution.</p>
           </div>

        </ol>

        <h3>3. Practical Application</h3>
        <p>Suppose you have a classifier that was tested on 10 samples, with 8 correct predictions and 2 incorrect predictions.</p>
        <ol type="a">
            <li>Model the classifier’s accuracy using a beta distribution. Specify the parameters α and β, and write the corresponding probability density function.</li>
            <div class="answer">
                <p>Assuming a uniform prior (Beta(1, 1)), which represents minimal prior knowledge:</p>
                <p>Number of successes (correct predictions) k = 8.<br>
                   Number of failures (incorrect predictions) n-k = 2.<br>
                   Total trials n = 10.</p>
                <p>The posterior parameters are:</p>
                <p><code>α = α₀ + k = 1 + 8 = 9</code><br>
                   <code>β = β₀ + (n - k) = 1 + 2 = 3</code></p>
                <p>So, the accuracy can be modeled by a <strong>Beta(9, 3)</strong> distribution.</p>
                <p>The probability density function (PDF) for a Beta(α, β) distribution is:</p>
                <p><code>f(x; α, β) = [Γ(α + β) / (Γ(α) * Γ(β))] * x^(α-1) * (1-x)^(β-1)</code> for 0 ≤ x ≤ 1</p>
                <p>Where Γ is the Gamma function (Γ(n) = (n-1)! for integer n).</p>
                <p>Substituting α=9, β=3:</p>
                <p><code>Γ(α + β) = Γ(12) = 11!</code><br>
                   <code>Γ(α) = Γ(9) = 8!</code><br>
                   <code>Γ(β) = Γ(3) = 2! = 2</code></p>
                <p><code>Constant term = 11! / (8! * 2!) = (11 * 10 * 9 * 8!) / (8! * 2) = (11 * 10 * 9) / 2 = 990 / 2 = 495</code></p>
                <p>So, the PDF is:<br>
                   <strong><code>f(x; 9, 3) = 495 * x^8 * (1-x)^2</code></strong> for 0 ≤ x ≤ 1.
                </p>
            </div>

            <li>Compute the expected value (mean) and variance of this beta distribution. Interpret what these values tell you about the classifier’s performance.</li>
            <div class="answer">
                <p>For a Beta(α, β) distribution:</p>
                <p><strong>Expected Value (Mean):</strong><br>
                   <code>E[X] = α / (α + β) = 9 / (9 + 3) = 9 / 12 = 0.75</code>
                </p>
                <p><strong>Variance:</strong><br>
                   <code>Var[X] = (α * β) / [ (α + β)^2 * (α + β + 1) ]</code><br>
                   <code>Var[X] = (9 * 3) / [ (9 + 3)^2 * (9 + 3 + 1) ]</code><br>
                   <code>Var[X] = 27 / [ (12)^2 * 13 ] = 27 / (144 * 13) = 27 / 1872 ≈ 0.0144</code>
                </p>
                <p><strong>Interpretation:</strong>
                    <ul>
                        <li>The <strong>expected value (0.75)</strong> represents our best point estimate for the classifier's true accuracy based on the observed data and the assumed prior. It suggests that, on average, we expect the classifier to be correct 75% of the time. Note that this is slightly lower than the observed accuracy (8/10 = 0.8) due to the influence of the uniform prior (which pulls the estimate away from the extremes of 0 and 1).</li>
                        <li>The <strong>variance (≈ 0.0144)</strong> quantifies the uncertainty around this estimate. A relatively small variance indicates moderate confidence, but since it's based on only 10 samples (+2 pseudo-samples from the prior), there's still noticeable uncertainty. More data would typically decrease the variance, leading to a more precise estimate. (The standard deviation is sqrt(0.0144) ≈ 0.12).</li>
                    </ul>
                </p>
            </div>

            <li>Using this beta distribution, calculate the probability that the classifier’s true accuracy is greater than 0.85. (You may leave your answer in terms of an integral or use a statistical table if provided.)</li>
            <div class="answer">
                 <p>We need to calculate P(X > 0.85) where X ~ Beta(9, 3). This involves integrating the PDF from 0.85 to 1:</p>
                 <p><code>P(X > 0.85) = ∫[from 0.85 to 1] f(x; 9, 3) dx</code></p>
                 <p><code>P(X > 0.85) = ∫[from 0.85 to 1] 495 * x^8 * (1-x)^2 dx</code></p>
                 <p>This integral represents the area under the Beta(9, 3) curve between x = 0.85 and x = 1.0.</p>
                 <p><em>(Calculation note: This integral evaluates to approximately 0.275. This would typically be computed using statistical software or the regularized incomplete beta function, Iₓ(α, β).)</em></p>
                 <p><strong>Answer (as integral):</strong> <code>∫[0.85 to 1] 495 * x^8 * (1-x)^2 dx</code></p>
            </div>

            <li>If you were to collect 5 more samples and the classifier achieved 4 correct predictions, how would the beta distribution parameters update? What does this suggest about the stability of the classifier’s accuracy?</li>
            <div class="answer">
                <p>Current posterior distribution (treated as the new prior): Beta(9, 3)</p>
                <p>New data: 5 samples, k_new = 4 correct, n_new - k_new = 1 incorrect.</p>
                <p>Update the parameters:</p>
                <p><code>α_updated = α_current + k_new = 9 + 4 = 13</code><br>
                   <code>β_updated = β_current + (n_new - k_new) = 3 + 1 = 4</code></p>
                <p>The new distribution modeling the accuracy is <strong>Beta(13, 4)</strong>.</p>
                <p><strong>Interpretation of Stability:</strong>
                    <ul>
                        <li>The expected value of the new distribution is E[X] = 13 / (13 + 4) = 13 / 17 ≈ 0.765. This is slightly higher than the previous estimate (0.75), reflecting the reasonably good performance on the new batch (4/5 = 0.8).</li>
                        <li>The variance of the new distribution is Var[X] = (13 * 4) / [ (17)^2 * (17 + 1) ] = 52 / (289 * 18) = 52 / 5202 ≈ 0.0100. This variance is lower than the previous variance (≈ 0.0144).</li>
                        <li>The increase in the expected value and, more importantly, the decrease in variance suggest that our estimate of the classifier's accuracy is becoming <strong>more stable</strong> and precise as we incorporate more data. The accuracy estimate is converging based on the accumulated evidence.</li>
                    </ul>
                </p>
            </div>

            <li>I have tested a classifier over 10 different customers, and i have measured an accuracy of 0.7. Now I have a new customer. How should I think about testing the classifier on the new customer? How many examples are "enough"? </li>
             <div class="answer">
                 <p><strong>Thinking about testing on the new customer:</strong></p>
                 <ul>
                    <li><strong>Prior Belief from Past Data:</strong> The average accuracy of 0.7 across 10 different customers provides valuable prior information. We can think of this prior belief in terms of a Beta distribution. The mean of this distribution would be 0.7 (so α / (α + β) = 0.7).</li>
                    <li><strong>Strength of the Prior (Beta Parameters):</strong> The "strength" of this prior (represented by the sum α + β) depends on how much information we think the previous 10 customers provide about this *new* customer. This depends critically on how <strong>similar</strong> we believe the new customer is to the old ones.
                        <ul>
                            <li>If we believe the new customer is <strong>very similar</strong> to the previous 10, we might use a relatively strong prior. For instance, if the 0.7 accuracy was based on many samples per customer, we could construct a Beta prior with a large α+β, such that α/(α+β)=0.7 (e.g., Beta(14, 6) implying N=α+β=20 "effective prior trials", or even stronger like Beta(35, 15) if we have high confidence in the similarity). A strong prior means our initial estimate for the new customer will be close to 0.7, and it will take more new data to shift this estimate significantly.</li>
                            <li>If we believe the new customer might be <strong>significantly different</strong>, or if the performance varied wildly among the first 10 customers, we should use a weaker prior. This means choosing smaller α and β such that α/(α+β)=0.7 (e.g., Beta(7, 3) corresponding to N=10, or even weaker like Beta(3.5, 1.5) or defaulting towards a less informative Beta(1, 1) uniform prior). A weak prior allows the data gathered specifically from the new customer to quickly dominate the accuracy estimate.</li>
                        </ul>
                    </li>
                    <li><strong>Customer Variability:</strong> Acknowledge that even with a prior, performance might vary. The 0.7 average might hide significant variation across the initial customers.</li>
                    <li><strong>Necessity of New Data:</strong> Regardless of the prior strength, it's crucial to test the classifier specifically on data *from this new customer*. This new data will update the prior Beta distribution to a posterior distribution using Bayesian updating: `α_posterior = α_prior + k_new`, `β_posterior = β_prior + failures_new`.</li>
                    <li><strong>Potential Need for Adaptation:</strong> If posterior accuracy (after testing on new data) is poor, consider if the model needs fine-tuning or adaptation using data specific to that customer.</li>
                 </ul>
                 <p><strong>How many examples are "enough"?</strong></p>
                 <p>There's no single magic number. "Enough" examples are needed to obtain a posterior Beta distribution that is sufficiently precise for your needs. Factors influencing this include:</p>
                 <ul>
                    <li><strong>Desired Confidence/Precision:</strong> How narrow does the credibility interval (e.g., 95% interval from the posterior Beta distribution) for the accuracy need to be? More samples narrow the interval.</li>
                    <li><strong>Strength of the Prior:</strong> A stronger prior requires fewer new samples to reach a certain level of precision compared to a weak prior, assuming the prior is reasonably accurate.</li>
                    <li><strong>Observed Performance & Variability:</strong> Consistent performance on the new samples allows for quicker convergence.</li>
                    <li><strong>Cost of Errors:</strong> High costs necessitate higher confidence and thus potentially more samples.</li>
                    <li><strong>Cost/Availability of Labeled Data:</strong> Practical constraints.</li>
                 </ul>
                 <p><strong>Practical Approach:</strong> Define your prior (Beta(α₀, β₀)) based on similarity assessment. Start collecting data from the new customer (e.g., batch of 20-50). Update the Beta distribution. Check the posterior mean and variance/credibility interval. If the estimate is precise enough for decision-making, stop. Otherwise, collect more data and repeat until the desired precision is achieved or practical limits are reached.</p>
            </div>


            <li>Somebody gives you 100 000 data points. You have to train and test a binary classifier. How do you decide your train/test split </li>
             <div class="answer">
                 <p>With a large dataset like 100,000 points, the strategy for splitting involves ensuring adequate data for training, validation, and reliable testing.</p>
                 <ol>
                     <li><strong>Three-Way Split (Train/Validation/Test):</strong> This is the standard best practice.
                         <ul>
                             <li><strong>Training Set:</strong> Used to fit the model parameters.</li>
                             <li><strong>Validation Set:</strong> Used for hyperparameter tuning and model selection.</li>
                             <li><strong>Test Set:</strong> Used *only once* at the end for final, unbiased performance evaluation.</li>
                         </ul>
                     </li>
                     <li><strong>Determining Test Set Size (Absolute Number):</strong> The critical factor for the test set is its <strong>absolute size</strong>, not its percentage of the total. The absolute number of samples determines the statistical power and precision (e.g., width of confidence intervals) of your performance evaluation.
                         <ul>
                            <li>A larger absolute test set size leads to more reliable and precise estimates of generalization performance (e.g., lower uncertainty about the true accuracy).</li>
                            <li>For example, evaluating on 1,000 test samples might give an accuracy estimate with a 95% confidence interval of +/- 2%, while 10,000 samples might narrow that to +/- 0.6% (actual values depend on the accuracy level).</li>
                            <li>With 100,000 total points, you can afford a relatively large absolute test set. Aim for a size that provides the desired level of confidence in your results. Common choices for large datasets range from <strong>5,000 to 20,000</strong> samples. Let's choose <strong>10,000</strong> samples for the test set as a robust number.</li>
                         </ul>
                     </li>
                     <li><strong>Determining Validation Set Size:</strong> The validation set also benefits from having a sufficient absolute number of samples to reliably detect performance differences when tuning hyperparameters. A size similar to or slightly larger/smaller than the test set is often used. Let's allocate another <strong>10,000</strong> samples for the validation set.</li>
                     <li><strong>Determining Training Set Size:</strong> The training set consists of the remaining data points.
                         <ul>
                            <li><code>Training Size = Total Size - Validation Size - Test Size</code></li>
                            <li><code>Training Size = 100,000 - 10,000 - 10,000 = 80,000</code></li>
                         </ul>
                         This large training set (80,000 samples) is beneficial for training potentially complex models.
                     </li>
                     <li><strong>Resulting Split Ratio:</strong> This allocation (80k Train / 10k Validation / 10k Test) corresponds to an 80%/10%/10% split, but the decision process prioritized securing adequate absolute numbers for validation and testing first.</li>
                     <li><strong>Stratification & Randomization:</strong> Ensure the split is done randomly (unless data is temporal) and is <strong>stratified</strong>. Stratification maintains the original proportion of the positive and negative classes in each of the train, validation, and test sets, which is crucial for representative evaluation, especially with imbalanced data.</li>
                     <li><strong>Temporal Data Consideration:</strong> If the data has a time dependency, splits must be chronological (train on past, validate on middle, test on most recent) rather than random.</li>
                 </ol>
                 <p><strong>Decision Summary:</strong> Prioritize determining the absolute number of samples needed for reliable validation and testing based on desired precision (e.g., 10,000 for validation, 10,000 for testing). Allocate the remaining substantial portion (80,000) to training. Always use stratified random splitting unless the data is temporal.</p>
            </div>


        </ol>
    </div>

    <div class="answer">
        <h3>Fairness in Loan Repayment Classification</h3>
      
        <p>Ensuring fairness in a classifier that predicts loan repayment likelihood is crucial due to the significant impact loan decisions have on individuals' lives and the potential for perpetuating historical biases. Given the classifier outputs scores leading to True Positives (TP - correctly predict repay), True Negatives (TN - correctly predict default), False Positives (FP - incorrectly predict repay, actual default), and False Negatives (FN - incorrectly predict default, actual repay), we can define several criteria to measure fairness across groups of interest (e.g., based on race, gender, age, ethnicity).</p>
      
        <h4>Potential Fairness Criteria:</h4>
        <p>Different criteria capture different notions of fairness, and often, satisfying one might conflict with satisfying another. Key criteria include:</p>
        <ul>
            <li>
              <strong>Demographic Parity (Statistical Parity):</strong>
              <ul>
                  <li><strong>Definition:</strong> The likelihood of being approved for a loan (predicted positive outcome) is the same regardless of group membership.</li>
                  <li><strong>Metric:</strong> Rate(Predicted Repay | Group A) = Rate(Predicted Repay | Group B). Mathematically: <code>(TP_A + FP_A) / N_A = (TP_B + FP_B) / N_B</code>, where N is the total number of individuals in the group.</li>
                  <li><strong>Focus:</strong> Equalizes approval rates across groups.</li>
                  <li><strong>Limitation:</strong> Ignores whether applicants are actually qualified. If base repayment rates differ between groups, achieving this parity might require approving unqualified individuals or denying qualified ones.</li>
              </ul>
            </li>
            <li>
              <strong>Equal Opportunity:</strong>
              <ul>
                  <li><strong>Definition:</strong> Individuals who can actually repay the loan (true positives) should have an equal probability of being approved, regardless of their group.</li>
                  <li><strong>Metric:</strong> True Positive Rate (TPR) or Recall is equal across groups. <code>TPR_A = TPR_B</code>. Mathematically: <code>TP_A / (TP_A + FN_A) = TP_B / (TP_B + FN_B)</code>.</li>
                  <li><strong>Focus:</strong> Ensures qualified applicants have the same chance of success (getting the loan).</li>
                  <li><strong>Limitation:</strong> Doesn't constrain error rates for unqualified applicants (False Positive Rate might differ).</li>
              </ul>
            </li>
             <li>
              <strong>Equalized Odds:</strong>
              <ul>
                  <li><strong>Definition:</strong> This is stricter than Equal Opportunity. It requires both the True Positive Rate (TPR) and the False Positive Rate (FPR) to be equal across groups.</li>
                  <li><strong>Metric:</strong> <code>TPR_A = TPR_B</code> AND <code>FPR_A = FPR_B</code>. FPR is <code>FP / (FP + TN)</code> (the rate at which unqualified applicants are wrongly approved).</li>
                  <li><strong>Focus:</strong> Ensures qualified applicants have equal opportunity (TPR equality) AND unqualified applicants are misclassified at equal rates (FPR equality). Tries to balance opportunity and risk assessment parity.</li>
                  <li><strong>Limitation:</strong> Can be very hard to achieve simultaneously, especially if base rates differ.</li>
              </ul>
            </li>
             <li>
              <strong>Predictive Rate Parity (or Predictive Parity):</strong>
              <ul>
                  <li><strong>Definition:</strong> Among the individuals approved for a loan, the proportion who actually repay should be the same regardless of group.</li>
                  <li><strong>Metric:</strong> Positive Predictive Value (PPV) or Precision is equal across groups. <code>PPV_A = PPV_B</code>. Mathematically: <code>TP_A / (TP_A + FP_A) = TP_B / (TP_B + FP_B)</code>.</li>
                  <li><strong>Focus:</strong> Ensures the lender's prediction of repayment is equally reliable across groups for approved loans.</li>
                   <li><strong>Limitation:</strong> Can lead to unequal TPRs if base rates or score distributions differ.</li>
             </ul>
            </li>
             <li>
              <strong>Calibration:</strong>
              <ul>
                  <li><strong>Definition:</strong> Among individuals who receive the same prediction score from the classifier, the actual probability of repaying the loan should be the same, regardless of group.</li>
                  <li><strong>Metric:</strong> P(Actual Repay | Score=s, Group A) = P(Actual Repay | Score=s, Group B) for all scores 's'.</li>
                  <li><strong>Focus:</strong> Ensures the score has the same meaning in terms of repayment probability across groups.</li>
                  <li><strong>Limitation:</strong> Does not guarantee equal approval rates or equal error rates overall.</li>
              </ul>
            </li>
        </ul>
      
        <h4>Approach to Addressing Fairness:</h4>
        <ol>
            <li><strong>Define Groups & Metrics:</strong> Clearly identify the protected attributes (e.g., race, gender) defining the groups. Choose the fairness criterion (or criteria) most relevant to the specific legal, ethical, and business context. Be explicit about why a particular definition is chosen.</li>
            <li><strong>Audit & Measure:</strong> Train the classifier and evaluate its performance and the chosen fairness metric(s) on a representative dataset, calculating the metrics separately for each group.</li>
            <li><strong>Identify Disparities:</strong> Compare the fairness metrics across groups. Determine if the observed differences are statistically significant and practically meaningful (exceeding a pre-defined tolerance).</li>
            <li><strong>Mitigation (If Necessary):</strong> If unacceptable disparities are found, apply fairness mitigation techniques:
                <ul>
                    <li><em>Pre-processing:</em> Modify the training data to reduce bias (e.g., re-sampling, re-weighting instances based on group).</li>
                    <li><em>In-processing:</em> Incorporate fairness constraints directly into the model training objective function (e.g., adding regularization terms that penalize fairness metric violations).</li>
                    <li><em>Post-processing:</em> Adjust the classifier's output scores or decision threshold differently for different groups to achieve the desired fairness metric outcome. *Caution: This can sometimes be legally problematic ("disparate treatment").*</li>
                </ul>
            </li>
            <li><strong>Evaluate Trade-offs:</strong> Re-evaluate the classifier's performance (overall accuracy, profit/loss) and fairness metrics after applying mitigation. Fairness interventions often involve a trade-off with predictive accuracy. Document these trade-offs to inform decision-making.</li>
            <li><strong>Iterate & Monitor:</strong> Fairness is not a one-time fix. Continuously monitor the classifier's performance and fairness metrics in production as data distributions might shift over time.</li>
        </ol>
      
        <h4>Potential Challenges:</h4>
        <ul>
            <li><strong>Conflicting Definitions:</strong> It's often mathematically impossible to satisfy multiple fairness criteria simultaneously (e.g., Demographic Parity, Equal Opportunity, and Calibration) unless base rates are equal across groups or the classifier is perfect. Choosing which definition to prioritize is a significant challenge.</li>
            <li><strong>Data Bias:</strong> The training data may reflect historical societal biases (e.g., past discriminatory lending practices). Models trained on this data can learn and perpetuate or even amplify these biases.</li>
            <li><strong>Measurement Reliability:</strong> Requires sufficient data for each subgroup to reliably calculate fairness metrics. Small or intersectional groups (e.g., based on race *and* gender) pose statistical challenges.</li>
            <li><strong>Accuracy-Fairness Trade-off:</strong> Techniques used to improve fairness might reduce the overall predictive accuracy or profitability of the classifier.</li>
            <li><strong>Legal and Ethical Complexity:</strong> The legal definition of fairness and discrimination (e.g., disparate impact vs. disparate treatment) is complex and evolving. Some mitigation strategies (like post-processing adjustments) might face legal scrutiny.</li>
            <li><strong>Implementation Difficulty:</strong> Fairness-aware machine learning techniques are an active area of research and can be complex to implement correctly.</li>
             <li><strong>Long-Term Impact:</strong> The downstream effects of fairness interventions need careful consideration.</li>
       </ul>
      </div>

    <div class="section">
        <h2>Section 3: Clustering Metrics</h2>

        <h3>5. Definitions and Interpretation</h3>
        <ol type="a">
            <li>Define purity, homogeneity, and completeness as clustering evaluation metrics.</li>
            <div class="answer">
                <p>These metrics evaluate clustering quality by comparing cluster assignments to ground truth class labels.</p>
                <p><strong>Purity:</strong>
                    <ul>
                        <li>For each cluster, identify the most frequent ground truth class label among its members (the majority class).</li>
                        <li>Purity is the fraction of correctly assigned data points, assuming each cluster is assigned the label of its majority class.</li>
                        <li>Formula: <code>Purity = (1/N) * Σ [over k clusters] max |Ck ∩ Ci|</code> where N is total points, Ck is the set of points in cluster k, Ci is the set of points in class i, and max is taken over all classes i.</li>
                        <li>Value range: [0, 1]. Higher is better.</li>
                    </ul>
                </p>
                <p><strong>Homogeneity:</strong>
                    <ul>
                        <li>Measures whether each cluster contains *only* members of a single class.</li>
                        <li>A clustering is perfectly homogeneous if all its clusters are uni-class (contain points from only one class).</li>
                        <li>It's based on conditional entropy: <code>Homogeneity = 1 - H(Classes | Clusters) / H(Classes)</code> if H(Classes) > 0, else 1.</li>
                        <li>Value range: [0, 1]. Higher is better.</li>
                    </ul>
                </p>
                <p><strong>Completeness:</strong>
                    <ul>
                        <li>Measures whether all members of a given class are assigned to the *same* cluster.</li>
                        <li>A clustering is perfectly complete if all points belonging to a given class are elements of the same cluster.</li>
                        <li>It's based on conditional entropy: <code>Completeness = 1 - H(Clusters | Classes) / H(Clusters)</code> if H(Clusters) > 0, else 1.</li>
                        <li>Value range: [0, 1]. Higher is better.</li>
                    </ul>
                </p>
                 <p><em>Note: H(Y|X) is the conditional entropy of Y given X, and H(Y) is the entropy of Y.</em></p>
           </div>

            <li>Explain one key limitation of purity as a metric and how homogeneity or completeness might address this limitation.</li>
             <div class="answer">
                 <p><strong>Key Limitation of Purity:</strong> Purity does not penalize having a large number of clusters. A trivial clustering where every single data point is assigned to its own cluster achieves a perfect purity score of 1.0, as each cluster contains only one point (which trivially belongs to a single majority class). However, such a clustering is usually useless as it doesn't group data meaningfully.</p>
                 <p><strong>How Homogeneity/Completeness Address This:</strong>
                     <ul>
                         <li><strong>Homogeneity:</strong> While the trivial one-point-per-cluster solution is perfectly homogeneous (each cluster contains only members of a single class), it doesn't necessarily lead to the highest *combined* score when considering completeness.</li>
                         <li><strong>Completeness:</strong> The trivial one-point-per-cluster solution achieves very low completeness if the true classes contain more than one member, because members of the same class are split across many different clusters. Completeness explicitly penalizes this splitting of classes.</li>
                         <li>Metrics like the <strong>V-measure</strong> (the harmonic mean of homogeneity and completeness) provide a balanced evaluation that addresses the shortcomings of purity. A high V-measure requires both high homogeneity (clusters are internally consistent class-wise) and high completeness (classes aren't split across clusters). The trivial solution gets perfect homogeneity but terrible completeness, resulting in a low V-measure.</li>
                     </ul>
                 </p>
            </div>
        </ol>

        <h3>6. Problem Solving</h3>
        <p>Consider a clustering result with 3 clusters and the following contingency table comparing cluster assignments to true labels:</p>
        <table>
            <tr>
                <th>Cluster</th>
                <th>Class A</th>
                <th>Class B</th>
                <th>Class C</th>
                <th>Cluster Total</th>
            </tr>
            <tr>
                <td>C1</td>
                <td>40</td>
                <td>10</td>
                <td>5</td>
                <td>55</td>
            </tr>
            <tr>
                <td>C2</td>
                <td>5</td>
                <td>30</td>
                <td>5</td>
                <td>40</td>
            </tr>
            <tr>
                <td>C3</td>
                <td>10</td>
                <td>5</td>
                <td>40</td>
                <td>55</td>
            </tr>
             <tr>
                <td><strong>Class Total</strong></td>
                <td><strong>55</strong></td>
                <td><strong>45</strong></td>
                <td><strong>50</strong></td>
                <td><strong>N=150</strong></td>
            </tr>
        </table>
        <ol type="a">
            <li>Compute the purity of the clustering. Show your work.</li>
            <div class="answer">
                <p>Identify the majority class count in each cluster:</p>
                <ul>
                    <li>Cluster C1: Max count is 40 (Class A).</li>
                    <li>Cluster C2: Max count is 30 (Class B).</li>
                    <li>Cluster C3: Max count is 40 (Class C).</li>
                </ul>
                 <p>Sum these maximum counts:</p>
                 <p><code>Sum of max counts = 40 + 30 + 40 = 110</code></p>
                 <p>Divide by the total number of data points (N = 150):</p>
                 <p><code>Purity = (Sum of max counts) / N = 110 / 150</code></p>
                 <p><code>Purity = 11 / 15 ≈ 0.733</code></p>
                 <p><strong>Purity ≈ 0.733</strong></p>
            </div>

            <li>Calculate the homogeneity of the clustering using entropy-based measures. (Hint: Use base-2 logarithm for entropy calculations.)</li>
            <div class="answer">
                <p>Homogeneity = 1 - H(Classes | Clusters) / H(Classes)</p>
                <p><strong>1. Calculate H(Classes):</strong></p>
                <ul>
                    <li>P(A) = 55/150, P(B) = 45/150, P(C) = 50/150</li>
                    <li>H(Classes) = - [P(A)log2(P(A)) + P(B)log2(P(B)) + P(C)log2(P(C))]</li>
                    <li>H(Classes) = - [(55/150)log2(55/150) + (45/150)log2(45/150) + (50/150)log2(50/150)]</li>
                    <li>H(Classes) ≈ - [0.367*(-1.448) + 0.3*(-1.737) + 0.333*(-1.585)]</li>
                    <li>H(Classes) ≈ - [-0.531 - 0.521 - 0.528] ≈ 1.580 bits</li>
                </ul>
                <p><strong>2. Calculate H(Classes | Clusters):</strong></p>
                <p>H(Classes | Clusters) = Σ [ |Ck| / N * H(Classes | Ck) ]</p>
                <ul>
                    <li><strong>H(Classes | C1):</strong> (|C1|=55). P(A|C1)=40/55, P(B|C1)=10/55, P(C|C1)=5/55
                       <ul><li>H(Classes|C1) = -[(40/55)log2(40/55) + (10/55)log2(10/55) + (5/55)log2(5/55)] ≈ 1.096 bits</li></ul>
                    </li>
                     <li><strong>H(Classes | C2):</strong> (|C2|=40). P(A|C2)=5/40, P(B|C2)=30/40, P(C|C2)=5/40
                       <ul><li>H(Classes|C2) = -[(5/40)log2(5/40) + (30/40)log2(30/40) + (5/40)log2(5/40)] ≈ 1.061 bits</li></ul>
                    </li>
                     <li><strong>H(Classes | C3):</strong> (|C3|=55). P(A|C3)=10/55, P(B|C3)=5/55, P(C|C3)=40/55
                       <ul><li>H(Classes|C3) = -[(10/55)log2(10/55) + (5/55)log2(5/55) + (40/55)log2(40/55)] ≈ 1.096 bits</li></ul>
                    </li>
                </ul>
                 <p>H(Classes | Clusters) = (55/150)*1.096 + (40/150)*1.061 + (55/150)*1.096</p>
                 <p>H(Classes | Clusters) ≈ 0.367*1.096 + 0.267*1.061 + 0.367*1.096</p>
                 <p>H(Classes | Clusters) ≈ 0.402 + 0.283 + 0.402 ≈ 1.087 bits</p>
                <p><strong>3. Calculate Homogeneity:</strong></p>
                 <p>Homogeneity = 1 - (1.087 / 1.580) ≈ 1 - 0.688 ≈ 0.312</p>
                 <p><strong>Homogeneity ≈ 0.312</strong></p>
            </div>

            <li>Calculate the completeness of the clustering.</li>
            <div class="answer">
                <p>Completeness = 1 - H(Clusters | Classes) / H(Clusters)</p>
                 <p><strong>1. Calculate H(Clusters):</strong></p>
                 <ul>
                    <li>P(C1) = 55/150, P(C2) = 40/150, P(C3) = 55/150</li>
                    <li>H(Clusters) = - [P(C1)log2(P(C1)) + P(C2)log2(P(C2)) + P(C3)log2(P(C3))]</li>
                    <li>H(Clusters) = - [(55/150)log2(55/150) + (40/150)log2(40/150) + (55/150)log2(55/150)]</li>
                    <li>H(Clusters) ≈ - [0.367*(-1.448) + 0.267*(-1.904) + 0.367*(-1.448)]</li>
                    <li>H(Clusters) ≈ - [-0.531 - 0.508 - 0.531] ≈ 1.570 bits</li>
                 </ul>
                 <p><strong>2. Calculate H(Clusters | Classes):</strong></p>
                 <p>H(Clusters | Classes) = Σ [ |Ci| / N * H(Clusters | Ci) ]</p>
                 <ul>
                    <li><strong>H(Clusters | Class A):</strong> (|A|=55). P(C1|A)=40/55, P(C2|A)=5/55, P(C3|A)=10/55
                        <ul><li>H(Clusters|A) = -[(40/55)log2(40/55) + (5/55)log2(5/55) + (10/55)log2(10/55)] ≈ 1.096 bits</li></ul>
                    </li>
                     <li><strong>H(Clusters | Class B):</strong> (|B|=45). P(C1|B)=10/45, P(C2|B)=30/45, P(C3|B)=5/45
                        <ul><li>H(Clusters|B) = -[(10/45)log2(10/45) + (30/45)log2(30/45) + (5/45)log2(5/45)] ≈ 1.224 bits</li></ul>
                    </li>
                    <li><strong>H(Clusters | Class C):</strong> (|C|=50). P(C1|C)=5/50, P(C2|C)=5/50, P(C3|C)=40/50
                        <ul><li>H(Clusters|C) = -[(5/50)log2(5/50) + (5/50)log2(5/50) + (40/50)log2(40/50)] ≈ 0.922 bits</li></ul>
                    </li>
                 </ul>
                 <p>H(Clusters | Classes) = (55/150)*1.096 + (45/150)*1.224 + (50/150)*0.922</p>
                 <p>H(Clusters | Classes) ≈ 0.367*1.096 + 0.3*1.224 + 0.333*0.922</p>
                 <p>H(Clusters | Classes) ≈ 0.402 + 0.367 + 0.307 ≈ 1.076 bits</p>
                 <p><strong>3. Calculate Completeness:</strong></p>
                 <p>Completeness = 1 - (1.076 / 1.570) ≈ 1 - 0.685 ≈ 0.315</p>
                 <p><strong>Completeness ≈ 0.315</strong></p>
            </div>

            <li>Based on your results, discuss whether this clustering is more “pure” or more “homogeneous.” What might this imply about the clustering algorithm’s performance? (Extra credit question)</li>
            <div class="answer">
                <p><strong>Comparison:</strong>
                    <ul>
                        <li>Purity ≈ 0.733</li>
                        <li>Homogeneity ≈ 0.312</li>
                        <li>Completeness ≈ 0.315</li>
                    </ul>
                    The clustering result has a significantly higher Purity score compared to its Homogeneity and Completeness scores, which are nearly identical and quite low. Therefore, the clustering is much more "pure" than it is "homogeneous" (or complete).
                </p>
                <p><strong>Implication about Performance:</strong>
                    <ul>
                        <li><strong>High Purity:</strong> Indicates that each cluster (C1, C2, C3) is dominated by a single class (A, B, C respectively). The core of each cluster likely corresponds well to one of the true classes.</li>
                        <li><strong>Low Homogeneity:</strong> Indicates that the clusters are not truly "pure" in the strict sense; they contain significant numbers of points from other classes (e.g., C1 contains 10 B's and 5 C's besides the dominant 40 A's). The clusters are mixed.</li>
                         <li><strong>Low Completeness:</strong> Indicates that members of the same true class are spread across multiple clusters (e.g., Class A points are found in C1, C2, and C3). The algorithm failed to group all members of each class together.</li>
                    </ul>
                </p>
                <p><strong>Conclusion:</strong> The high purity score alone could be misleading. While the algorithm successfully identified the dominant class for each cluster's core, it struggled with class boundaries. The low homogeneity and completeness suggest that the clusters are quite mixed and the true classes are fragmented across different clusters. This might imply that the true classes are not well-separated or have shapes that are difficult for the chosen clustering algorithm (e.g., perhaps k-means, which prefers spherical clusters) to capture accurately, leading to significant mixing near cluster peripheries or fragmentation of non-convex classes.</p>
            </div>
        </ol>
    </div>

    <p><strong>End of Exam</strong></p>
</body>
</html>