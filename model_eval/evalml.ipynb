{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9d8e0c9-2c0e-4f8b-8f1e-1bfa4f5a1a8c",
   "metadata": {},
   "source": [
    "# Evaluating Machine Learning Classification and Clustering Models\n",
    "\n",
    "Machine learning models must be evaluated with appropriate metrics to ensure they perform well and generalize to new data. In this tutorial, we will demonstrate how to evaluate both classification and clustering models using the Iris dataset. We will cover common evaluation metrics, discuss pitfalls, and explain best practices.\n",
    "\n",
    "We'll use **Python** and **scikit-learn** throughout this notebook (no TensorFlow)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637c8fa8-411f-4122-8ad1-7cb48450a468",
   "metadata": {},
   "source": [
    "## 1. Dataset and Setup\n",
    "\n",
    "For this tutorial, we will use the classic **Iris dataset**. The Iris dataset contains 150 samples with 4 features (sepal length, sepal width, petal length, petal width) and 3 classes (Setosa, Versicolor, Virginica). It is balanced (50 samples per class) and is well-suited for both classification and clustering demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21b688aa-4d1f-4f08-9d2f-df2bd0ecf59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (150, 4)\n",
      "Label distribution: [50 50 50]\n",
      "Class names: ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data        # feature matrix (150 x 4)\n",
    "y = iris.target      # labels (0, 1, 2 for the three species)\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Label distribution:\", np.bincount(y))\n",
    "print(\"Class names:\", iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d3e6ce-0df0-4e1f-9e89-7e972ce0b29d",
   "metadata": {},
   "source": [
    "## 2. Classification Model Evaluation\n",
    "\n",
    "We will first train a simple **Logistic Regression** classifier on the Iris dataset. We'll split the data into training and testing sets (70% training, 30% testing) and evaluate the classifier using several metrics:\n",
    "\n",
    "- **Accuracy**\n",
    "- **Precision**\n",
    "- **Recall**\n",
    "- **F1-score**\n",
    "- **Confusion Matrix**\n",
    "- **ROC Curve / AUC** (using one-vs-rest for one class)\n",
    "\n",
    "Let's begin by training our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0f5e26d-dc14-41c5-a3a1-3aab68c8a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Train a Logistic Regression classifier\n",
    "clf = LogisticRegression(max_iter=200, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c0a7b96-d320-45b2-887f-8d3b1c41f403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.933\n",
      "Precision (macro-average): 0.935\n",
      "Recall (macro-average): 0.933\n",
      "F1-score (macro-average): 0.933\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, average='macro')\n",
    "rec = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(f\"Precision (macro-average): {prec:.3f}\")\n",
    "print(f\"Recall (macro-average): {rec:.3f}\")\n",
    "print(f\"F1-score (macro-average): {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8571f8b-f2f3-4d3d-9a1e-635b47f87b4f",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "A **confusion matrix** breaks down how many samples of each true class were correctly or incorrectly predicted. It helps to visualize which classes are getting confused by the model.\n",
    "\n",
    "Let's compute and print the confusion matrix for our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5034a53-7eea-4a46-82a5-0d1a0c42e5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[15  0  0]\n",
      " [ 0 14  1]\n",
      " [ 0  2 13]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760d0cde-0f34-45cb-a6bd-42b2b5d80dc2",
   "metadata": {},
   "source": [
    "### ROC Curve and AUC (for one class)\n",
    "\n",
    "For binary classification, the ROC (Receiver Operating Characteristic) curve visualizes the True Positive Rate vs. False Positive Rate at various thresholds. In multi-class scenarios, we can compute a ROC curve for one class using a one-vs-rest approach. Here, we'll compute the ROC curve and AUC for class **virginica** (label 2).\n",
    "\n",
    "First, we need the predicted probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d98a00b5-fb15-4a46-9d37-0f0053eac1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for virginica vs. rest: 0.9933333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Get predicted probabilities for the test set\n",
    "y_prob = clf.predict_proba(X_test)\n",
    "\n",
    "# For class 'virginica' (label 2), create binary labels (1 if virginica, else 0)\n",
    "y_test_binary = (y_test == 2).astype(int)\n",
    "y_score = y_prob[:, 2]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test_binary, y_score)\n",
    "auc_score = roc_auc_score(y_test_binary, y_score)\n",
    "print(\"AUC for virginica vs. rest:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a40884-fd5a-4f9b-8797-1e0e14a4fae3",
   "metadata": {},
   "source": [
    "## 3. Clustering Model Evaluation\n",
    "\n",
    "Now, let's evaluate clustering performance. We'll use **K-Means** clustering on the Iris dataset (ignoring the true labels during clustering) and then evaluate the clusters using internal and external metrics:\n",
    "\n",
    "- **Silhouette Score**: Measures how similar an object is to its own cluster versus other clusters (values range from -1 to 1, higher is better).\n",
    "- **Davies-Bouldin Index (DBI)**: Evaluates the average similarity between each cluster and its most similar one (lower is better).\n",
    "- **Adjusted Rand Index (ARI)**: Compares the clustering against the true labels (1.0 means perfect agreement).\n",
    "\n",
    "Let's perform K-Means clustering and compute these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9f3d223-3cc5-4c2a-8a0d-40b8b1e216ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.551\n",
      "Davies-Bouldin Index: 0.666\n",
      "Adjusted Rand Index (vs true labels): 0.716\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, adjusted_rand_score\n",
    "\n",
    "# Cluster the Iris data into 3 clusters\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Compute clustering metrics\n",
    "sil_score = silhouette_score(X, cluster_labels)\n",
    "db_index = davies_bouldin_score(X, cluster_labels)\n",
    "ari = adjusted_rand_score(y, cluster_labels)  # External metric comparing to true labels\n",
    "\n",
    "print(f\"Silhouette Score: {sil_score:.3f}\")\n",
    "print(f\"Davies-Bouldin Index: {db_index:.3f}\")\n",
    "print(f\"Adjusted Rand Index (vs true labels): {ari:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9a5c4-37c7-4d5b-b4cc-6b9cbbd889be",
   "metadata": {},
   "source": [
    "## 4. Common Pitfalls in Model Evaluation\n",
    "\n",
    "Even with the right metrics, there are several common pitfalls in evaluating ML models:\n",
    "\n",
    "### Class Imbalance\n",
    "- **Issue:** High accuracy can be misleading if one class dominates the dataset.\n",
    "- **Solution:** Use metrics such as precision, recall, F1-score, and analyze the confusion matrix.\n",
    "\n",
    "### Overfitting\n",
    "- **Issue:** A model may perform excellently on training data but poorly on unseen data.\n",
    "- **Solution:** Always evaluate on a hold-out test set or using cross-validation.\n",
    "\n",
    "### Data Leakage\n",
    "- **Issue:** Unintentional use of test data in training (e.g., via preprocessing) can inflate performance metrics.\n",
    "- **Solution:** Ensure strict separation between training and testing data and fit preprocessing steps on training data only.\n",
    "\n",
    "### Metric Misuse\n",
    "- **Issue:** Relying on a single metric (like accuracy) or using the wrong metric for the problem can lead to misinterpretation.\n",
    "- **Solution:** Choose metrics that align with the business goal or research question, and consider multiple evaluation perspectives (e.g., confusion matrices, ROC curves, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e8f167-3d6a-464d-9df2-dbb4789d1d3d",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this tutorial, we demonstrated how to evaluate machine learning models for both classification and clustering tasks using the Iris dataset. We covered metrics such as accuracy, precision, recall, F1-score, ROC/AUC for classification, and silhouette score, Davies-Bouldin index, and Adjusted Rand Index for clustering. Additionally, we discussed common pitfalls including class imbalance, overfitting, data leakage, and metric misuse.\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Always use multiple metrics to get a complete picture of model performance.\n",
    "- Be cautious of pitfalls such as data leakage and overfitting.\n",
    "- For clustering, rely on both internal metrics (like silhouette score) and external metrics (if labels are available) to assess performance.\n",
    "\n",
    "Evaluating models correctly is critical for deploying reliable and trustworthy AI systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
